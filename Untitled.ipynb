{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# vim: set fileencoding=utf-8 :\n",
    "# @author: Tiago de Freitas Pereira <tiago.pereira@idiap.ch>\n",
    "# @date: Wed 11 May 2016 09:39:36 CEST \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Simple script that trains a triplet net (Lenet as basis) with MNIST data\n",
    "Usage:\n",
    "  train_mnist_triplet.py [--batch-size=<arg> --iterations=<arg> --validation-interval=<arg>]\n",
    "  train_mnist_triplet.py -h | --help\n",
    "Options:\n",
    "  -h --help     Show this screen.\n",
    "  --batch-size=<arg>  [default: 1]\n",
    "  --iterations=<arg>  [default: 30000]\n",
    "  --validation-interval=<arg>  [default: 100]\n",
    "\"\"\"\n",
    "\n",
    "#from docopt import docopt\n",
    "import tensorflow as tf\n",
    "import util\n",
    "from DataShuffler import DataShuffler\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from lenet import Lenet\n",
    "\n",
    "SEED = 10\n",
    "\n",
    "\n",
    "def compute_euclidean_distance(x, y):\n",
    "    \"\"\"\n",
    "    Computes the euclidean distance between two tensorflow variables\n",
    "    \"\"\"\n",
    "\n",
    "    d = tf.square(tf.sub(x, y))\n",
    "    d = tf.sqrt(tf.reduce_sum(d)) # What about the axis ???\n",
    "    return d\n",
    "\n",
    "\n",
    "def compute_triplet_loss(anchor_feature, positive_feature, negative_feature, margin):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the contrastive loss as in\n",
    "    L = || f_a - f_p ||^2 - || f_a - f_n ||^2 + m\n",
    "    **Parameters**\n",
    "     anchor_feature:\n",
    "     positive_feature:\n",
    "     negative_feature:\n",
    "     margin: Triplet margin\n",
    "    **Returns**\n",
    "     Return the loss operation\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope(\"triplet_loss\"):\n",
    "        d_p_squared = tf.square(compute_euclidean_distance(anchor_feature, positive_feature))\n",
    "        d_n_squared = tf.square(compute_euclidean_distance(anchor_feature, negative_feature))\n",
    "\n",
    "        loss = tf.maximum(0., d_p_squared - d_n_squared + margin)\n",
    "        print(d_p_squared - d_n_squared + margin)\n",
    "\n",
    "        return tf.reduce_mean(loss), tf.reduce_mean(d_p_squared), tf.reduce_mean(d_n_squared)\n",
    "\n",
    "\n",
    "def main():\n",
    "    #args = docopt(__doc__, version='Mnist training with TensorFlow')\n",
    "\n",
    "    #BATCH_SIZE = int(args['--batch-size'])\n",
    "    #ITERATIONS = int(args['--iterations'])\n",
    "    #VALIDATION_TEST = int(args['--validation-interval'])\n",
    "    BATCH_SIZE = 100\n",
    "    ITERATIONS = 200\n",
    "    VALIDATION_TEST = 5\n",
    "    perc_train = 0.9\n",
    "    MARGIN = 0.01\n",
    "\n",
    "    data, labels = util.load_mnist(data_dir=\"./src/bob.db.mnist/bob/db/mnist/\")\n",
    "    print(\"got data\")\n",
    "    print (\"{}\", len(data))\n",
    "    data_shuffler = DataShuffler(data, labels)\n",
    "\n",
    "    # Siamease place holders\n",
    "    train_anchor_data = tf.placeholder(tf.float32, shape=(BATCH_SIZE, 28, 28, 1), name=\"anchor\")\n",
    "    train_positive_data = tf.placeholder(tf.float32, shape=(BATCH_SIZE, 28, 28, 1), name=\"positive\")\n",
    "    train_negative_data = tf.placeholder(tf.float32, shape=(BATCH_SIZE, 28, 28, 1), name=\"negative\")\n",
    "    labels_anchor = tf.placeholder(tf.int32, shape=BATCH_SIZE)\n",
    "    labels_positive = tf.placeholder(tf.int32, shape=BATCH_SIZE)\n",
    "    labels_negative = tf.placeholder(tf.int32, shape=BATCH_SIZE)\n",
    "\n",
    "    #validation_data = tf.placeholder(tf.float32, shape=(data_shuffler.validation_data.shape[0], 28, 28, 1))\n",
    "\n",
    "    # Creating the architecture\n",
    "    lenet_architecture = Lenet(seed=SEED)\n",
    "    lenet_train_anchor = lenet_architecture.create_lenet(train_anchor_data)\n",
    "    lenet_train_positive = lenet_architecture.create_lenet(train_positive_data)\n",
    "    lenet_train_negative = lenet_architecture.create_lenet(train_negative_data)\n",
    "    #lenet_validation = lenet_architecture.create_lenet(validation_data, train=False)\n",
    "\n",
    "    loss, positives, negatives = compute_triplet_loss(lenet_train_anchor, lenet_train_positive, lenet_train_negative, MARGIN)\n",
    "\n",
    "\n",
    "    # Defining training parameters\n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.001, # Learning rate\n",
    "        batch * BATCH_SIZE,\n",
    "        data_shuffler.train_data.shape[0],\n",
    "        0.95 # Decay step\n",
    "    )\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=batch)\n",
    "    #validation_prediction = tf.nn.softmax(lenet_validation)\n",
    "\n",
    "    # Training\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        train_writer = tf.train.SummaryWriter('./logs_tensorboard/triplet/train',\n",
    "                                              session.graph)\n",
    "\n",
    "        test_writer = tf.train.SummaryWriter('./logs_tensorboard/triplet/test',\n",
    "                                             session.graph)\n",
    "\n",
    "        tf.scalar_summary('loss', loss)\n",
    "        tf.scalar_summary('positives', positives)\n",
    "        tf.scalar_summary('negatives', negatives)\n",
    "        tf.scalar_summary('lr', learning_rate)\n",
    "        merged = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "        tf.initialize_all_variables().run()\n",
    "        #pp = PdfPages(\"groups.pdf\")\n",
    "        for step in range(ITERATIONS):\n",
    "\n",
    "            batch_anchor, batch_positive, batch_negative, \\\n",
    "            batch_labels_anchor, batch_labels_positive, batch_labels_negative = \\\n",
    "                data_shuffler.get_triplet(n_labels=10, n_triplets=BATCH_SIZE)\n",
    "\n",
    "            feed_dict = {train_anchor_data: batch_anchor,\n",
    "                         train_positive_data: batch_positive,\n",
    "                         train_negative_data: batch_negative,\n",
    "                         labels_anchor: batch_labels_anchor,\n",
    "                         labels_positive: batch_labels_positive,\n",
    "                         labels_negative: batch_labels_negative\n",
    "            }\n",
    "\n",
    "            _, l, lr, summary = session.run([optimizer, loss, learning_rate, merged],\n",
    "                                                feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary, step)\n",
    "\n",
    "            if step % VALIDATION_TEST == 0:\n",
    "\n",
    "                batch_anchor, batch_positive, batch_negative, \\\n",
    "                batch_labels_anchor, batch_labels_positive, batch_labels_negative = \\\n",
    "                    data_shuffler.get_triplet(n_labels=10, n_triplets=BATCH_SIZE, is_target_set_train=False)\n",
    "\n",
    "                feed_dict = {train_anchor_data: batch_anchor,\n",
    "                             train_positive_data: batch_positive,\n",
    "                             train_negative_data: batch_negative,\n",
    "                             labels_anchor: batch_labels_anchor,\n",
    "                             labels_positive: batch_labels_positive,\n",
    "                             labels_negative: batch_labels_negative\n",
    "                             }\n",
    "\n",
    "                lv, summary = session.run([loss, merged], feed_dict=feed_dict)\n",
    "                test_writer.add_summary(summary, step)\n",
    "                print(\"Loss Validation {0}\".format(lv))\n",
    "\n",
    "                #batch_train_data, batch_train_labels = data_shuffler.get_batch(\n",
    "                #    data_shuffler.validation_data.shape[0],\n",
    "                #    train_dataset=True)\n",
    "\n",
    "                #features_train = session.run(lenet_validation,\n",
    "                #                       feed_dict={validation_data: batch_train_data[:]})\n",
    "\n",
    "                #batch_validation_data, batch_validation_labels = data_shuffler.get_batch(data_shuffler.validation_data.shape[0],\n",
    "                #                                                                         train_dataset=False)\n",
    "\n",
    "                #features_validation = session.run(lenet_validation,\n",
    "                #                       feed_dict={validation_data: batch_validation_data[:]})\n",
    "\n",
    "                #accuracy = util.compute_accuracy(features_train, batch_train_labels, features_validation, batch_validation_labels, 10)\n",
    "                #print(\"Step {0}. Loss = {1}, Lr={2}, Acc = {3}\".\n",
    "                #      format(step, l, lr, accuracy))\n",
    "\n",
    "                #fig = util.plot_embedding_lda(features_validation, batch_validation_labels)\n",
    "\n",
    "                #pp.savefig(fig)\n",
    "\n",
    "        #print(\"Step {0}. Loss = {1}, Lr={2}, Acc = {3}\".\n",
    "              #format(step, l, lr, accuracy))\n",
    "        #print(\"End !!\")\n",
    "        #pp.close()\n",
    "\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got data\n",
      "{} 70000\n",
      "Tensor(\"triplet_loss/add_1:0\", shape=(), dtype=float32)\n",
      "Loss Validation 0.0\n",
      "Loss Validation 0.0\n",
      "Loss Validation 0.0\n",
      "Loss Validation 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-48f18d9931e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             _, l, lr, summary = session.run([optimizer, loss, learning_rate, merged],\n\u001b[0;32m--> 143\u001b[0;31m                                                 feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
